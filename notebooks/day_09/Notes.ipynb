{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas Operations\n",
    "\n",
    "In this lecture, we'll dive deep into advanced Pandas operations. We'll cover complex data manipulation techniques, advanced indexing, grouping, merging, and performance optimization. By the end of this lecture, you'll have a strong grasp of these powerful Pandas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Data Selection and Filtering\n",
    "\n",
    "Pandas provides powerful tools for selecting and filtering data. We'll explore boolean indexing, .loc, .iloc, and complex conditional selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B     C\n",
      "0  1  10   foo\n",
      "1  2  20   bar\n",
      "2  3  30   baz\n",
      "3  4  40   qux\n",
      "4  5  50  quux\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': range(1, 6),\n",
    "    'B': range(10, 60, 10),\n",
    "    'C': ['foo', 'bar', 'baz', 'qux', 'quux']\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B     C\n",
      "2  3  30   baz\n",
      "3  4  40   qux\n",
      "4  5  50  quux\n",
      "   A   B    C\n",
      "0  1  10  foo\n",
      "1  2  20  bar\n"
     ]
    }
   ],
   "source": [
    "# Select rows where A is greater than 2\n",
    "print(df[df['A'] > 2])\n",
    "\n",
    "# Select rows where B is less than 40 and C is not 'baz'\n",
    "print(df[(df['B'] < 40) & (df['C'] != 'baz')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .loc and .iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A    C\n",
      "1  2  bar\n",
      "2  3  baz\n",
      "3  4  qux\n",
      "   A    C\n",
      "1  2  bar\n",
      "2  3  baz\n"
     ]
    }
   ],
   "source": [
    "# .loc for label-based indexing\n",
    "print(df.loc[1:3, ['A', 'C']])\n",
    "\n",
    "# .iloc for integer-based indexing\n",
    "print(df.iloc[1:3, [0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Selection with Multiple Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B     C\n",
      "0  1  10   foo\n",
      "2  3  30   baz\n",
      "3  4  40   qux\n",
      "4  5  50  quux\n"
     ]
    }
   ],
   "source": [
    "# Complex condition\n",
    "condition = (df['A'] > 2) & (df['B'] < 50) | (df['C'].isin(['foo', 'quux']))\n",
    "print(df[condition])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Transformation\n",
    "\n",
    "Pandas offers various methods for transforming data, including apply(), applymap(), and lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A                               4\n",
      "B                              40\n",
      "C    Max length: 4, Min length: 3\n",
      "D    Max length: 4, Min length: 3\n",
      "dtype: object\n",
      "0    N/A\n",
      "1    N/A\n",
      "2    N/A\n",
      "3    N/A\n",
      "4    N/A\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply a function to each column\n",
    "def column_operation(col):\n",
    "    if col.dtype == 'object':  # For non-numeric columns\n",
    "        return f\"Max length: {col.str.len().max()}, Min length: {col.str.len().min()}\"\n",
    "    else:  # For numeric columns\n",
    "        return col.max() - col.min()\n",
    "\n",
    "print(df.apply(column_operation))\n",
    "\n",
    "# Apply a function to each row\n",
    "print(df.apply(lambda row: row['A'] * row['B'] if pd.api.types.is_numeric_dtype(row['A']) and pd.api.types.is_numeric_dtype(row['B']) else \"N/A\", axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applymap() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B     C\n",
      "0  1  10   FOO\n",
      "1  2  20   BAR\n",
      "2  3  30   BAZ\n",
      "3  4  40   QUX\n",
      "4  5  50  QUUX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/0/ru.iiec.pydroid3/cache/ipykernel_9616/3131992841.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(df.applymap(lambda x: str(x).upper()))\n"
     ]
    }
   ],
   "source": [
    "# Apply a function to every element\n",
    "print(df.applymap(lambda x: str(x).upper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns Based on Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B     C     D\n",
      "0  1  10   foo   Low\n",
      "1  2  20   bar   Low\n",
      "2  3  30   baz   Low\n",
      "3  4  40   qux  High\n",
      "4  5  50  quux  High\n"
     ]
    }
   ],
   "source": [
    "# Create a new column based on a condition\n",
    "df['D'] = np.where(df['A'] > 3, 'High', 'Low')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouping and Aggregation\n",
    "\n",
    "GroupBy operations allow you to split your data into groups, apply functions, and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category  Value1  Value2\n",
      "0        A      10     100\n",
      "1        B      20     200\n",
      "2        A      30     300\n",
      "3        B      40     400\n",
      "4        A      50     500\n",
      "5        B      60     600\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame for demonstration\n",
    "df2 = pd.DataFrame({\n",
    "    'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Value1': [10, 20, 30, 40, 50, 60],\n",
    "    'Value2': [100, 200, 300, 400, 500, 600]\n",
    "})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic GroupBy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Value1  Value2\n",
      "Category                \n",
      "A           30.0   300.0\n",
      "B           40.0   400.0\n",
      "         Value1            Value2            \n",
      "           mean  sum count   mean   sum count\n",
      "Category                                     \n",
      "A          30.0   90     3  300.0   900     3\n",
      "B          40.0  120     3  400.0  1200     3\n"
     ]
    }
   ],
   "source": [
    "# Group by Category and calculate mean\n",
    "print(df2.groupby('Category').mean())\n",
    "\n",
    "# Group by Category and apply multiple aggregations\n",
    "print(df2.groupby('Category').agg(['mean', 'sum', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Aggregation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    mean                                          median                                             std\n",
      "Category                                                                                                                                                \n",
      "A         Value1     30.0\n",
      "Value2    300.0\n",
      "dtype: float64  Value1     30.0\n",
      "Value2    300.0\n",
      "dtype: float64  Value1     20.0\n",
      "Value2    200.0\n",
      "dtype: float64\n",
      "B         Value1     40.0\n",
      "Value2    400.0\n",
      "dtype: float64  Value1     40.0\n",
      "Value2    400.0\n",
      "dtype: float64  Value1     20.0\n",
      "Value2    200.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Custom aggregation function\n",
    "def custom_agg(x):\n",
    "    return pd.Series({\n",
    "        'mean': x.mean(),\n",
    "        'median': x.median(),\n",
    "        'std': x.std()\n",
    "    })\n",
    "\n",
    "print(df2.groupby('Category').apply(custom_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables and Cross-tabulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product       A    B\n",
      "Date                \n",
      "2023-01-01  100  200\n",
      "2023-01-02  150  250\n",
      "Product       A    B\n",
      "Date                \n",
      "2023-01-01  100  200\n",
      "2023-01-02  150  250\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table\n",
    "df3 = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n",
    "    'Product': ['A', 'B', 'A', 'B'],\n",
    "    'Sales': [100, 200, 150, 250]\n",
    "})\n",
    "\n",
    "pivot = pd.pivot_table(df3, values='Sales', index='Date', columns='Product', aggfunc='sum')\n",
    "print(pivot)\n",
    "\n",
    "# Cross-tabulation\n",
    "print(pd.crosstab(df3['Date'], df3['Product'], values=df3['Sales'], aggfunc='sum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data\n",
    "\n",
    "Dealing with missing data is a common task in data analysis. Pandas provides various methods to handle missing values effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B    C\n",
      "0  1.0  10.0    a\n",
      "1  2.0   NaN    b\n",
      "2  NaN  30.0    c\n",
      "3  4.0  40.0  NaN\n",
      "4  5.0  50.0    e\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with missing values\n",
    "df4 = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, 30, 40, 50],\n",
    "    'C': ['a', 'b', 'c', np.nan, 'e']\n",
    "})\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B  C\n",
      "0  1.0  10.0  a\n",
      "1  2.0   0.0  b\n",
      "2  0.0  30.0  c\n",
      "3  4.0  40.0  0\n",
      "4  5.0  50.0  e\n",
      "     A      B        C\n",
      "0  1.0   10.0        a\n",
      "1  2.0  100.0        b\n",
      "2  0.0   30.0        c\n",
      "3  4.0   40.0  Unknown\n",
      "4  5.0   50.0        e\n",
      "     A     B  C\n",
      "0  1.0  10.0  a\n",
      "1  2.0  10.0  b\n",
      "2  2.0  30.0  c\n",
      "3  4.0  40.0  c\n",
      "4  5.0  50.0  e\n",
      "     A     B  C\n",
      "0  1.0  10.0  a\n",
      "1  2.0  30.0  b\n",
      "2  4.0  30.0  c\n",
      "3  4.0  40.0  e\n",
      "4  5.0  50.0  e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/0/ru.iiec.pydroid3/cache/ipykernel_9616/3862585993.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  print(df4.fillna(method='ffill'))\n",
      "/data/user/0/ru.iiec.pydroid3/cache/ipykernel_9616/3862585993.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  print(df4.fillna(method='bfill'))\n"
     ]
    }
   ],
   "source": [
    "# Fill with a specific value\n",
    "print(df4.fillna(0))\n",
    "\n",
    "# Fill with different values for each column\n",
    "print(df4.fillna({'A': 0, 'B': 100, 'C': 'Unknown'}))\n",
    "\n",
    "# Fill with forward fill method\n",
    "print(df4.fillna(method='ffill'))\n",
    "\n",
    "# Fill with backward fill method\n",
    "print(df4.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/0/ru.iiec.pydroid3/cache/ipykernel_9616/373601297.py:2: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  print(df4.interpolate())\n",
      "/data/user/0/ru.iiec.pydroid3/cache/ipykernel_9616/373601297.py:5: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  print(df4.interpolate(method='polynomial', order=2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B    C\n",
      "0  1.0  10.0    a\n",
      "1  2.0  20.0    b\n",
      "2  3.0  30.0    c\n",
      "3  4.0  40.0  NaN\n",
      "4  5.0  50.0    e\n",
      "     A     B    C\n",
      "0  1.0  10.0    a\n",
      "1  2.0  20.0    b\n",
      "2  3.0  30.0    c\n",
      "3  4.0  40.0  NaN\n",
      "4  5.0  50.0    e\n"
     ]
    }
   ],
   "source": [
    "# Linear interpolation\n",
    "print(df4.interpolate())\n",
    "\n",
    "# Polynomial interpolation\n",
    "print(df4.interpolate(method='polynomial', order=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data in Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-01    1.0\n",
      "2023-01-02    NaN\n",
      "2023-01-03    3.0\n",
      "2023-01-04    NaN\n",
      "2023-01-05    5.0\n",
      "2023-01-06    6.0\n",
      "Freq: D, dtype: float64\n",
      "2023-01-01    1.0\n",
      "2023-01-02    2.0\n",
      "2023-01-03    3.0\n",
      "2023-01-04    4.0\n",
      "2023-01-05    5.0\n",
      "2023-01-06    6.0\n",
      "Freq: D, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a time series with missing data\n",
    "dates = pd.date_range('20230101', periods=6)\n",
    "ts = pd.Series([1, np.nan, 3, np.nan, 5, 6], index=dates)\n",
    "print(ts)\n",
    "\n",
    "# Time-aware interpolation\n",
    "print(ts.interpolate(method='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merging and Joining DataFrames\n",
    "\n",
    "Pandas provides powerful tools for combining different DataFrames. We'll explore various types of joins and concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left DataFrame:\n",
      "  key  value\n",
      "0   A      1\n",
      "1   B      2\n",
      "2   C      3\n",
      "3   D      4\n",
      "\n",
      "Right DataFrame:\n",
      "  key  value\n",
      "0   B     20\n",
      "1   D     40\n",
      "2   E     50\n",
      "3   F     60\n"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrames\n",
    "df_left = pd.DataFrame({'key': ['A', 'B', 'C', 'D'], 'value': [1, 2, 3, 4]})\n",
    "df_right = pd.DataFrame({'key': ['B', 'D', 'E', 'F'], 'value': [20, 40, 50, 60]})\n",
    "print(\"Left DataFrame:\")\n",
    "print(df_left)\n",
    "print(\"\\nRight DataFrame:\")\n",
    "print(df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join:\n",
      "  key  value_x  value_y\n",
      "0   B        2       20\n",
      "1   D        4       40\n",
      "\n",
      "Outer Join:\n",
      "  key  value_x  value_y\n",
      "0   A      1.0      NaN\n",
      "1   B      2.0     20.0\n",
      "2   C      3.0      NaN\n",
      "3   D      4.0     40.0\n",
      "4   E      NaN     50.0\n",
      "5   F      NaN     60.0\n",
      "\n",
      "Left Join:\n",
      "  key  value_x  value_y\n",
      "0   A        1      NaN\n",
      "1   B        2     20.0\n",
      "2   C        3      NaN\n",
      "3   D        4     40.0\n",
      "\n",
      "Right Join:\n",
      "  key  value_x  value_y\n",
      "0   B      2.0       20\n",
      "1   D      4.0       40\n",
      "2   E      NaN       50\n",
      "3   F      NaN       60\n"
     ]
    }
   ],
   "source": [
    "# Inner join\n",
    "print(\"Inner Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='inner'))\n",
    "\n",
    "# Outer join\n",
    "print(\"\\nOuter Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='outer'))\n",
    "\n",
    "# Left join\n",
    "print(\"\\nLeft Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='left'))\n",
    "\n",
    "# Right join\n",
    "print(\"\\nRight Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     value_left  value_right\n",
      "key                         \n",
      "A           1.0          NaN\n",
      "B           2.0         20.0\n",
      "C           3.0          NaN\n",
      "D           4.0         40.0\n",
      "E           NaN         50.0\n",
      "F           NaN         60.0\n"
     ]
    }
   ],
   "source": [
    "# Set 'key' as index\n",
    "df_left.set_index('key', inplace=True)\n",
    "df_right.set_index('key', inplace=True)\n",
    "\n",
    "# Merge on index\n",
    "print(df_left.join(df_right, how='outer', lsuffix='_left', rsuffix='_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical Concatenation:\n",
      "     value\n",
      "key       \n",
      "A        1\n",
      "B        2\n",
      "C        3\n",
      "D        4\n",
      "B       20\n",
      "D       40\n",
      "E       50\n",
      "F       60\n",
      "\n",
      "Horizontal Concatenation:\n",
      "     value  value\n",
      "key              \n",
      "A      1.0    NaN\n",
      "B      2.0   20.0\n",
      "C      3.0    NaN\n",
      "D      4.0   40.0\n",
      "E      NaN   50.0\n",
      "F      NaN   60.0\n"
     ]
    }
   ],
   "source": [
    "# Vertical concatenation\n",
    "print(\"Vertical Concatenation:\")\n",
    "print(pd.concat([df_left, df_right]))\n",
    "\n",
    "# Horizontal concatenation\n",
    "print(\"\\nHorizontal Concatenation:\")\n",
    "print(pd.concat([df_left, df_right], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Operations\n",
    "\n",
    "Pandas excels at handling time series data. We'll explore resampling, rolling windows, and other time-based operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               value\n",
      "2023-01-01 -1.173940\n",
      "2023-01-02 -0.154302\n",
      "2023-01-03 -0.059296\n",
      "2023-01-04  1.212269\n",
      "2023-01-05 -0.249615\n"
     ]
    }
   ],
   "source": [
    "# Create a time series DataFrame\n",
    "dates = pd.date_range('20230101', periods=100, freq='D')\n",
    "ts = pd.Series(np.random.randn(len(dates)), index=dates)\n",
    "df_ts = pd.DataFrame({'value': ts})\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               value\n",
      "2023-01-31 -0.033523\n",
      "2023-02-28 -0.164839\n",
      "2023-03-31 -0.090123\n",
      "2023-04-30  0.156954\n",
      "               value\n",
      "2023-01-01 -1.173940\n",
      "2023-01-08 -0.675408\n",
      "2023-01-15  0.357052\n",
      "2023-01-22  0.228703\n",
      "2023-01-29 -0.106586\n",
      "2023-02-05 -0.668872\n",
      "2023-02-12 -1.702220\n",
      "2023-02-19 -0.823237\n",
      "2023-02-26 -0.963081\n",
      "2023-03-05  0.153435\n",
      "2023-03-12  1.946141\n",
      "2023-03-19 -3.736399\n",
      "2023-03-26 -0.989691\n",
      "2023-04-02 -2.221668\n",
      "2023-04-09  3.127429\n",
      "2023-04-16  0.369376\n"
     ]
    }
   ],
   "source": [
    "# Resample to monthly frequency\n",
    "print(df_ts.resample('M').mean())\n",
    "\n",
    "# Resample to weekly frequency with sum\n",
    "print(df_ts.resample('W').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               value\n",
      "2023-01-01       NaN\n",
      "2023-01-02       NaN\n",
      "2023-01-03       NaN\n",
      "2023-01-04       NaN\n",
      "2023-01-05       NaN\n",
      "2023-01-06       NaN\n",
      "2023-01-07 -0.243904\n",
      "2023-01-08 -0.096487\n",
      "2023-01-09  0.036076\n",
      "2023-01-10  0.046044\n",
      "            value\n",
      "2023-01-01    NaN\n",
      "2023-01-02    NaN\n",
      "2023-01-03    NaN\n",
      "2023-01-04    NaN\n",
      "2023-01-05    NaN\n",
      "2023-01-06    NaN\n",
      "2023-01-07    NaN\n",
      "2023-01-08    NaN\n",
      "2023-01-09    NaN\n",
      "2023-01-10    NaN\n"
     ]
    }
   ],
   "source": [
    "# 7-day rolling mean\n",
    "print(df_ts.rolling(window=7).mean().head(10))\n",
    "\n",
    "# 30-day rolling standard deviation\n",
    "print(df_ts.rolling(window=30).std().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting and Lagging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               value\n",
      "2023-01-01       NaN\n",
      "2023-01-02 -1.173940\n",
      "2023-01-03 -0.154302\n",
      "2023-01-04 -0.059296\n",
      "2023-01-05  1.212269\n",
      "               value\n",
      "2023-01-01 -0.154302\n",
      "2023-01-02 -0.059296\n",
      "2023-01-03  1.212269\n",
      "2023-01-04 -0.249615\n",
      "2023-01-05  0.019245\n"
     ]
    }
   ],
   "source": [
    "# Shift data forward by 1 day\n",
    "print(df_ts.shift(1).head())\n",
    "\n",
    "# Shift data backward by 1 day\n",
    "print(df_ts.shift(-1).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and Time Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               value  year  month  day  dayofweek\n",
      "2023-01-01 -1.173940  2023      1    1          6\n",
      "2023-01-02 -0.154302  2023      1    2          0\n",
      "2023-01-03 -0.059296  2023      1    3          1\n",
      "2023-01-04  1.212269  2023      1    4          2\n",
      "2023-01-05 -0.249615  2023      1    5          3\n"
     ]
    }
   ],
   "source": [
    "# Extract various date components\n",
    "df_ts['year'] = df_ts.index.year\n",
    "df_ts['month'] = df_ts.index.month\n",
    "df_ts['day'] = df_ts.index.day\n",
    "df_ts['dayofweek'] = df_ts.index.dayofweek\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization\n",
    "\n",
    "When working with large datasets, optimizing performance becomes crucial. We'll explore some techniques to improve the efficiency of your Pandas operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Categorical Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization:\n",
      "Index             72\n",
      "id           4000000\n",
      "category    30000000\n",
      "dtype: int64\n",
      "\n",
      "Memory usage after optimization:\n",
      "Index            72\n",
      "id          4000000\n",
      "category    1000216\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a large DataFrame with repeated values\n",
    "df_large = pd.DataFrame({\n",
    "    'id': np.arange(1000000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 1000000)\n",
    "})\n",
    "\n",
    "# Check memory usage\n",
    "print(\"Memory usage before optimization:\")\n",
    "print(df_large.memory_usage(deep=True))\n",
    "\n",
    "# Convert to categorical\n",
    "df_large['category'] = df_large['category'].astype('category')\n",
    "\n",
    "# Check memory usage after optimization\n",
    "print(\"\\nMemory usage after optimization:\")\n",
    "print(df_large.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with itertuples(): 6.1898 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df_sample = pd.DataFrame({\n",
    "    'A': np.random.randint(1, 100, 1000000),\n",
    "    'B': np.random.randint(1, 100, 1000000)\n",
    "})\n",
    "\n",
    "# Using itertuples() for efficient iteration\n",
    "start_time = time.time()\n",
    "for row in df_sample.itertuples():\n",
    "    _ = row.A + row.B\n",
    "end_time = time.time()\n",
    "print(f\"Time taken with itertuples(): {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# Using iterrows() (slower)\n",
    "start_time = time.time()\n",
    "for index, row in df_sample.iterrows():\n",
    "    _ = row['A'] + row['B']\n",
    "end_time = time.time()\n",
    "print(f\"Time taken with iterrows(): {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-vectorized operation\n",
    "%time result = [x + y for x, y in zip(df_sample['A'], df_sample['B'])]\n",
    "\n",
    "# Vectorized operation\n",
    "%time result = df_sample['A'] + df_sample['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this comprehensive lecture on Advanced Pandas Operations, we've covered a wide range of topics including:\n",
    "\n",
    "1. Advanced Data Selection and Filtering\n",
    "2. Data Transformation\n",
    "3. Grouping and Aggregation\n",
    "4. Handling Missing Data\n",
    "5. Merging and Joining DataFrames\n",
    "6. Time Series Operations\n",
    "7. Performance Optimization\n",
    "\n",
    "These advanced techniques will allow you to manipulate and analyze complex datasets efficiently. Remember that practice is key to mastering these concepts. Try applying these techniques to your own datasets and experiment with different combinations of operations to solve real-world data problems.\n",
    "\n",
    "As you continue your journey with Pandas, keep exploring the official documentation and stay updated with new features and best practices in the data science community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "To reinforce your understanding of these advanced Pandas operations, try the following exercises:\n",
    "\n",
    "1. Create a DataFrame with at least 1000 rows and 5 columns of various data types. Perform complex filtering operations using boolean indexing and .loc/.iloc.\n",
    "\n",
    "2. Using the same DataFrame, create new columns based on complex conditions and apply custom functions using apply() and applymap().\n",
    "\n",
    "3. Perform advanced groupby operations with multiple columns and custom aggregation functions.\n",
    "\n",
    "4. Create a time series DataFrame and perform resampling, rolling window calculations, and time-based indexing operations.\n",
    "\n",
    "5. Merge multiple DataFrames using different join types and handle cases with missing data.\n",
    "\n",
    "6. Optimize a large DataFrame (>1 million rows) using categorical data types and vectorization techniques. Compare the performance before and after optimization.\n",
    "\n",
    "Good luck, and happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

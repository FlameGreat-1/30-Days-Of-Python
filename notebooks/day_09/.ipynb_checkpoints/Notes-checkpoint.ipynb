{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas Operations\n",
    "\n",
    "In this lecture, we'll dive deep into advanced Pandas operations. We'll cover complex data manipulation techniques, advanced indexing, grouping, merging, and performance optimization. By the end of this lecture, you'll have a strong grasp of these powerful Pandas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Data Selection and Filtering\n",
    "\n",
    "Pandas provides powerful tools for selecting and filtering data. We'll explore boolean indexing, .loc, .iloc, and complex conditional selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'A': range(1, 6),\n",
    "    'B': range(10, 60, 10),\n",
    "    'C': ['foo', 'bar', 'baz', 'qux', 'quux']\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where A is greater than 2\n",
    "print(df[df['A'] > 2])\n",
    "\n",
    "# Select rows where B is less than 40 and C is not 'baz'\n",
    "print(df[(df['B'] < 40) & (df['C'] != 'baz')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .loc and .iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc for label-based indexing\n",
    "print(df.loc[1:3, ['A', 'C']])\n",
    "\n",
    "# .iloc for integer-based indexing\n",
    "print(df.iloc[1:3, [0, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Selection with Multiple Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex condition\n",
    "condition = (df['A'] > 2) & (df['B'] < 50) | (df['C'].isin(['foo', 'quux']))\n",
    "print(df[condition])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Transformation\n",
    "\n",
    "Pandas offers various methods for transforming data, including apply(), applymap(), and lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a function to each column\n",
    "def column_operation(col):\n",
    "    return col.max() - col.min()\n",
    "\n",
    "print(df.apply(column_operation))\n",
    "\n",
    "# Apply a function to each row\n",
    "print(df.apply(lambda row: row['A'] * row['B'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applymap() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a function to every element\n",
    "print(df.applymap(lambda x: str(x).upper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns Based on Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column based on a condition\n",
    "df['D'] = np.where(df['A'] > 3, 'High', 'Low')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouping and Aggregation\n",
    "\n",
    "GroupBy operations allow you to split your data into groups, apply functions, and combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for demonstration\n",
    "df2 = pd.DataFrame({\n",
    "    'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'Value1': [10, 20, 30, 40, 50, 60],\n",
    "    'Value2': [100, 200, 300, 400, 500, 600]\n",
    "})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic GroupBy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Category and calculate mean\n",
    "print(df2.groupby('Category').mean())\n",
    "\n",
    "# Group by Category and apply multiple aggregations\n",
    "print(df2.groupby('Category').agg(['mean', 'sum', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Aggregation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation function\n",
    "def custom_agg(x):\n",
    "    return pd.Series({\n",
    "        'mean': x.mean(),\n",
    "        'median': x.median(),\n",
    "        'std': x.std()\n",
    "    })\n",
    "\n",
    "print(df2.groupby('Category').apply(custom_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables and Cross-tabulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table\n",
    "df3 = pd.DataFrame({\n",
    "    'Date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n",
    "    'Product': ['A', 'B', 'A', 'B'],\n",
    "    'Sales': [100, 200, 150, 250]\n",
    "})\n",
    "\n",
    "pivot = pd.pivot_table(df3, values='Sales', index='Date', columns='Product', aggfunc='sum')\n",
    "print(pivot)\n",
    "\n",
    "# Cross-tabulation\n",
    "print(pd.crosstab(df3['Date'], df3['Product'], values=df3['Sales'], aggfunc='sum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data\n",
    "\n",
    "Dealing with missing data is a common task in data analysis. Pandas provides various methods to handle missing values effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "df4 = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, 30, 40, 50],\n",
    "    'C': ['a', 'b', 'c', np.nan, 'e']\n",
    "})\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with a specific value\n",
    "print(df4.fillna(0))\n",
    "\n",
    "# Fill with different values for each column\n",
    "print(df4.fillna({'A': 0, 'B': 100, 'C': 'Unknown'}))\n",
    "\n",
    "# Fill with forward fill method\n",
    "print(df4.fillna(method='ffill'))\n",
    "\n",
    "# Fill with backward fill method\n",
    "print(df4.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation\n",
    "print(df4.interpolate())\n",
    "\n",
    "# Polynomial interpolation\n",
    "print(df4.interpolate(method='polynomial', order=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data in Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series with missing data\n",
    "dates = pd.date_range('20230101', periods=6)\n",
    "ts = pd.Series([1, np.nan, 3, np.nan, 5, 6], index=dates)\n",
    "print(ts)\n",
    "\n",
    "# Time-aware interpolation\n",
    "print(ts.interpolate(method='time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merging and Joining DataFrames\n",
    "\n",
    "Pandas provides powerful tools for combining different DataFrames. We'll explore various types of joins and concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "df_left = pd.DataFrame({'key': ['A', 'B', 'C', 'D'], 'value': [1, 2, 3, 4]})\n",
    "df_right = pd.DataFrame({'key': ['B', 'D', 'E', 'F'], 'value': [20, 40, 50, 60]})\n",
    "print(\"Left DataFrame:\")\n",
    "print(df_left)\n",
    "print(\"\\nRight DataFrame:\")\n",
    "print(df_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "print(\"Inner Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='inner'))\n",
    "\n",
    "# Outer join\n",
    "print(\"\\nOuter Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='outer'))\n",
    "\n",
    "# Left join\n",
    "print(\"\\nLeft Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='left'))\n",
    "\n",
    "# Right join\n",
    "print(\"\\nRight Join:\")\n",
    "print(pd.merge(df_left, df_right, on='key', how='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'key' as index\n",
    "df_left.set_index('key', inplace=True)\n",
    "df_right.set_index('key', inplace=True)\n",
    "\n",
    "# Merge on index\n",
    "print(df_left.join(df_right, how='outer', lsuffix='_left', rsuffix='_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical concatenation\n",
    "print(\"Vertical Concatenation:\")\n",
    "print(pd.concat([df_left, df_right]))\n",
    "\n",
    "# Horizontal concatenation\n",
    "print(\"\\nHorizontal Concatenation:\")\n",
    "print(pd.concat([df_left, df_right], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Operations\n",
    "\n",
    "Pandas excels at handling time series data. We'll explore resampling, rolling windows, and other time-based operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series DataFrame\n",
    "dates = pd.date_range('20230101', periods=100, freq='D')\n",
    "ts = pd.Series(np.random.randn(len(dates)), index=dates)\n",
    "df_ts = pd.DataFrame({'value': ts})\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to monthly frequency\n",
    "print(df_ts.resample('M').mean())\n",
    "\n",
    "# Resample to weekly frequency with sum\n",
    "print(df_ts.resample('W').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-day rolling mean\n",
    "print(df_ts.rolling(window=7).mean().head(10))\n",
    "\n",
    "# 30-day rolling standard deviation\n",
    "print(df_ts.rolling(window=30).std().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting and Lagging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift data forward by 1 day\n",
    "print(df_ts.shift(1).head())\n",
    "\n",
    "# Shift data backward by 1 day\n",
    "print(df_ts.shift(-1).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and Time Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract various date components\n",
    "df_ts['year'] = df_ts.index.year\n",
    "df_ts['month'] = df_ts.index.month\n",
    "df_ts['day'] = df_ts.index.day\n",
    "df_ts['dayofweek'] = df_ts.index.dayofweek\n",
    "print(df_ts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization\n",
    "\n",
    "When working with large datasets, optimizing performance becomes crucial. We'll explore some techniques to improve the efficiency of your Pandas operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Categorical Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large DataFrame with repeated values\n",
    "df_large = pd.DataFrame({\n",
    "    'id': np.arange(1000000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 1000000)\n",
    "})\n",
    "\n",
    "# Check memory usage\n",
    "print(\"Memory usage before optimization:\")\n",
    "print(df_large.memory_usage(deep=True))\n",
    "\n",
    "# Convert to categorical\n",
    "df_large['category'] = df_large['category'].astype('category')\n",
    "\n",
    "# Check memory usage after optimization\n",
    "print(\"\\nMemory usage after optimization:\")\n",
    "print(df_large.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df_sample = pd.DataFrame({\n",
    "    'A': range(1000000),\n",
    "    'B': range(1000000, 2000000)\n",
    "})\n",
    "\n",
    "# Using itertuples() for efficient iteration\n",
    "%time for row in df_sample.itertuples():\n",
    "    _ = row.A + row.B\n",
    "\n",
    "# Using iterrows() (slower)\n",
    "%time for index, row in df_sample.iterrows():\n",
    "    _ = row['A'] + row['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-vectorized operation\n",
    "%time result = [x + y for x, y in zip(df_sample['A'], df_sample['B'])]\n",
    "\n",
    "# Vectorized operation\n",
    "%time result = df_sample['A'] + df_sample['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this comprehensive lecture on Advanced Pandas Operations, we've covered a wide range of topics including:\n",
    "\n",
    "1. Advanced Data Selection and Filtering\n",
    "2. Data Transformation\n",
    "3. Grouping and Aggregation\n",
    "4. Handling Missing Data\n",
    "5. Merging and Joining DataFrames\n",
    "6. Time Series Operations\n",
    "7. Performance Optimization\n",
    "\n",
    "These advanced techniques will allow you to manipulate and analyze complex datasets efficiently. Remember that practice is key to mastering these concepts. Try applying these techniques to your own datasets and experiment with different combinations of operations to solve real-world data problems.\n",
    "\n",
    "As you continue your journey with Pandas, keep exploring the official documentation and stay updated with new features and best practices in the data science community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "To reinforce your understanding of these advanced Pandas operations, try the following exercises:\n",
    "\n",
    "1. Create a DataFrame with at least 1000 rows and 5 columns of various data types. Perform complex filtering operations using boolean indexing and .loc/.iloc.\n",
    "\n",
    "2. Using the same DataFrame, create new columns based on complex conditions and apply custom functions using apply() and applymap().\n",
    "\n",
    "3. Perform advanced groupby operations with multiple columns and custom aggregation functions.\n",
    "\n",
    "4. Create a time series DataFrame and perform resampling, rolling window calculations, and time-based indexing operations.\n",
    "\n",
    "5. Merge multiple DataFrames using different join types and handle cases with missing data.\n",
    "\n",
    "6. Optimize a large DataFrame (>1 million rows) using categorical data types and vectorization techniques. Compare the performance before and after optimization.\n",
    "\n",
    "Good luck, and happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


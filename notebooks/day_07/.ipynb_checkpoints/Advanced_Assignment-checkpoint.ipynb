{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas Assignment\n",
    "\n",
    "This notebook contains five advanced Pandas exercises with their complete solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Advanced Data Cleaning and Imputation\n",
    "\n",
    "You are given a dataset of customer information with missing values, outliers, and inconsistent data types. Your task is to clean the data, handle missing values using advanced imputation techniques, and prepare it for analysis.\n",
    "\n",
    "1. Load the dataset and display its information.\n",
    "2. Identify and handle outliers in numerical columns.\n",
    "3. Impute missing values using appropriate methods (mean, median, or advanced techniques like KNN imputation).\n",
    "4. Handle inconsistent data types and format issues.\n",
    "5. Create a function that automates this cleaning process for similar datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with issues\n",
    "df = pd.DataFrame({\n",
    "    'CustomerID': range(1, 1001),\n",
    "    'Age': np.random.randint(18, 90, 1000),\n",
    "    'Income': np.random.randint(20000, 200000, 1000),\n",
    "    'Credit_Score': np.random.randint(300, 850, 1000),\n",
    "    'Purchase_Amount': np.random.randint(100, 10000, 1000)\n",
    "})\n",
    "\n",
    "# Introduce missing values and outliers\n",
    "df.loc[np.random.choice(df.index, 100, replace=False), 'Age'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 100, replace=False), 'Income'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 50, replace=False), 'Credit_Score'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 20, replace=False), 'Age'] = df['Age'].max() * 2\n",
    "df.loc[np.random.choice(df.index, 20, replace=False), 'Income'] = df['Income'].max() * 10\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "def clean_customer_data(df):\n",
    "    # Make a copy of the dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handle outliers using IQR method\n",
    "    for column in ['Age', 'Income', 'Credit_Score', 'Purchase_Amount']:\n",
    "        Q1 = df_clean[column].quantile(0.25)\n",
    "        Q3 = df_clean[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean[column] = df_clean[column].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    # Impute missing values\n",
    "    df_clean['Age'].fillna(df_clean['Age'].median(), inplace=True)\n",
    "    df_clean['Income'].fillna(df_clean['Income'].mean(), inplace=True)\n",
    "    \n",
    "    # Use KNN imputation for Credit_Score\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_clean['Credit_Score'] = imputer.fit_transform(df_clean[['Credit_Score']])[:, 0]\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    df_clean['Age'] = df_clean['Age'].astype(int)\n",
    "    df_clean['Income'] = df_clean['Income'].astype(int)\n",
    "    df_clean['Credit_Score'] = df_clean['Credit_Score'].astype(int)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_cleaned = clean_customer_data(df)\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned.info())\n",
    "print(df_cleaned.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Time Series Analysis and Forecasting\n",
    "\n",
    "You have a dataset of daily stock prices for a company over the past 5 years. Your task is to analyze this time series data and create a simple forecasting model.\n",
    "\n",
    "1. Load and prepare the time series data.\n",
    "2. Perform time series decomposition to separate trend, seasonality, and residuals.\n",
    "3. Implement a simple moving average model for forecasting.\n",
    "4. Use an ARIMA model for more advanced forecasting.\n",
    "5. Evaluate and compare the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate sample stock price data\n",
    "dates = pd.date_range(start='2018-01-01', end='2022-12-31', freq='D')\n",
    "prices = np.cumsum(np.random.randn(len(dates))) + 100  # Random walk with drift\n",
    "df_stock = pd.DataFrame({'Date': dates, 'Price': prices})\n",
    "df_stock.set_index('Date', inplace=True)\n",
    "\n",
    "# Time series decomposition\n",
    "decomposition = seasonal_decompose(df_stock['Price'], model='additive', period=365)\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 16))\n",
    "decomposition.observed.plot(ax=ax1)\n",
    "ax1.set_title('Observed')\n",
    "decomposition.trend.plot(ax=ax2)\n",
    "ax2.set_title('Trend')\n",
    "decomposition.seasonal.plot(ax=ax3)\n",
    "ax3.set_title('Seasonal')\n",
    "decomposition.resid.plot(ax=ax4)\n",
    "ax4.set_title('Residual')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Simple Moving Average model\n",
    "def moving_average_forecast(series, window):\n",
    "    return series.rolling(window=window).mean()\n",
    "\n",
    "ma_forecast = moving_average_forecast(df_stock['Price'], window=30)\n",
    "\n",
    "# ARIMA model\n",
    "train = df_stock['Price'][:int(0.8*len(df_stock))]\n",
    "test = df_stock['Price'][int(0.8*len(df_stock)):]\n",
    "\n",
    "model = ARIMA(train, order=(1, 1, 1))\n",
    "results = model.fit()\n",
    "arima_forecast = results.forecast(steps=len(test))\n",
    "\n",
    "# Evaluate models\n",
    "ma_mse = mean_squared_error(test, ma_forecast[-len(test):])\n",
    "arima_mse = mean_squared_error(test, arima_forecast)\n",
    "\n",
    "print(f\"Moving Average MSE: {ma_mse}\")\n",
    "print(f\"ARIMA MSE: {arima_mse}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_stock.index, df_stock['Price'], label='Actual')\n",
    "plt.plot(df_stock.index, ma_forecast, label='Moving Average')\n",
    "plt.plot(test.index, arima_forecast, label='ARIMA')\n",
    "plt.legend()\n",
    "plt.title('Stock Price Forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Advanced Data Transformation and Analysis\n",
    "\n",
    "You have a large dataset of e-commerce transactions. Your task is to perform advanced data transformation and analysis to derive meaningful insights.\n",
    "\n",
    "1. Load the dataset and perform any necessary data cleaning.\n",
    "2. Use advanced groupby operations to analyze sales patterns.\n",
    "3. Implement a customer segmentation based on recency, frequency, and monetary value (RFM analysis).\n",
    "4. Create a function to identify the top products for each customer segment.\n",
    "5. Visualize the results using appropriate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample e-commerce data\n",
    "n_transactions = 100000\n",
    "df_ecommerce = pd.DataFrame({\n",
    "    'CustomerID': np.random.randint(1, 1001, n_transactions),\n",
    "    'Date': pd.date_range(start='2022-01-01', end='2022-12-31', periods=n_transactions),\n",
    "    'ProductID': np.random.randint(1, 101, n_transactions),\n",
    "    'Quantity': np.random.randint(1, 10, n_transactions),\n",
    "    'Price': np.random.uniform(10, 1000, n_transactions)\n",
    "})\n",
    "\n",
    "df_ecommerce['TotalAmount'] = df_ecommerce['Quantity'] * df_ecommerce['Price']\n",
    "\n",
    "# Advanced groupby operations\n",
    "sales_patterns = df_ecommerce.groupby([df_ecommerce['Date'].dt.month, 'ProductID'])['TotalAmount'].sum().unstack()\n",
    "print(\"Monthly sales patterns for each product:\")\n",
    "print(sales_patterns.head())\n",
    "\n",
    "# RFM Analysis\n",
    "today = df_ecommerce['Date'].max()\n",
    "rfm = df_ecommerce.groupby('CustomerID').agg({\n",
    "    'Date': lambda x: (today - x.max()).days,\n",
    "    'CustomerID': 'count',\n",
    "    'TotalAmount': 'sum'\n",
    "})\n",
    "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Segment customers\n",
    "r_labels = range(4, 0, -1)\n",
    "f_labels = range(1, 5)\n",
    "m_labels = range(1, 5)\n",
    "\n",
    "r_quartiles = pd.qcut(rfm['Recency'], q=4, labels=r_labels)\n",
    "f_quartiles = pd.qcut(rfm['Frequency'], q=4, labels=f_labels)\n",
    "m_quartiles = pd.qcut(rfm['Monetary'], q=4, labels=m_labels)\n",
    "\n",
    "rfm['R'] = r_quartiles\n",
    "rfm['F'] = f_quartiles\n",
    "rfm['M'] = m_quartiles\n",
    "\n",
    "rfm['RFM_Score'] = rfm['R'].astype(str) + rfm['F'].astype(str) + rfm['M'].astype(str)\n",
    "\n",
    "# Function to identify top products for each segment\n",
    "def top_products_by_segment(df, rfm, segment):\n",
    "    segment_customers = rfm[rfm['RFM_Score'] == segment].index\n",
    "    segment_transactions = df[df['CustomerID'].isin(segment_customers)]\n",
    "    top_products = segment_transactions.groupby('ProductID')['TotalAmount'].sum().nlargest(5)\n",
    "    return top_products\n",
    "\n",
    "# Example: Top products for the best customers (segment '444')\n",
    "best_customers_products = top_products_by_segment(df_ecommerce, rfm, '444')\n",
    "print(\"\\nTop 5 products for best customers:\")\n",
    "print(best_customers_products)\n",
    "\n",
    "# Visualize RFM segments\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Recency', y='Frequency', size='Monetary', data=rfm, hue='RFM_Score', palette='viridis')\n",
    "plt.title('Customer Segments based on RFM Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Advanced Data Visualization and Statistical Analysis\n",
    "\n",
    "You have a dataset containing information about employees in a large company. Your task is to perform advanced data visualization and statistical analysis to uncover insights about employee performance and satisfaction.\n",
    "\n",
    "1. Load and prepare the dataset.\n",
    "2. Create advanced visualizations to explore relationships between variables.\n",
    "3. Perform statistical tests to identify significant factors affecting employee performance.\n",
    "4. Implement a simple predictive model for employee satisfaction.\n",
    "5. Present your findings in a clear and concise manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate sample employee data\n",
    "n_employees = 1000\n",
    "df_employees = pd.DataFrame({\n",
    "    'Age': np.random.randint(22, 65, n_employees),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], n_employees),\n",
    "    'Education': np.random.choice(['Bachelor', 'Master', 'PhD'], n_employees),\n",
    "    'Experience': np.random.randint(0, 40, n_employees),\n",
    "    'Salary': np.random.randint(30000, 150000, n_employees),\n",
    "    'Department': np.random.choice(['HR', 'IT', 'Finance', 'Marketing', 'Operations'], n_employees),\n",
    "    'Performance': np.random.uniform(1, 5, n_employees),\n",
    "    'Satisfaction': np.random.uniform(1, 5, n_employees)\n",
    "})\n",
    "\n",
    "# Advanced visualizations\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Department', y='Salary', hue='Gender', data=df_employees)\n",
    "plt.title('Salary Distribution by Department and Gender')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Experience', y='Performance', hue='Education', size='Salary', data=df_employees)\n",
    "plt.title('Experience vs Performance by Education and Salary')\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "\n",
    "# T-test for performance difference between genders\n",
    "male_performance = df_employees[df_employees['Gender'] == 'Male']['Performance']\n",
    "female_performance = df_employees[df_employees['Gender'] == 'Female']['Performance']\n",
    "t_stat, p_value = stats.ttest_ind(male_performance, female_performance)\n",
    "print(f\"T-test for performance difference between genders: p-value = {p_value:.4f}\")\n",
    "\n",
    "# ANOVA for performance difference among departments\n",
    "departments = df_employees['Department'].unique()\n",
    "dept_performances = [df_employees[df_employees['Department'] == dept]['Performance'] for dept in departments]\n",
    "f_stat, p_value = stats.f_oneway(*dept_performances)\n",
    "print(f\"ANOVA for performance difference among departments: p-value = {p_value:.4f}\")\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = df_employees[['Age', 'Experience', 'Salary', 'Performance', 'Satisfaction']].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Predictive model for employee satisfaction\n",
    "X = df_employees[['Age', 'Experience', 'Salary', 'Performance']]\n",
    "y = df_employees['Satisfaction']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"\\nEmployee Satisfaction Prediction Model R-squared: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nFeature Importance:\")\n",
    "for feature, importance in zip(X.columns, model.coef_):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Big Data Processing with Pandas and Dask\n",
    "\n",
    "You have a very large dataset (several gigabytes) of customer  transactions that doesn't fit into memory. Your task is to process this data using Pandas and Dask to perform analysis and generate insights.\n",
    "\n",
    "1. Set up a Dask DataFrame to handle the large dataset.\n",
    "2. Perform basic exploratory data analysis using Dask.\n",
    "3. Implement a data processing pipeline that includes filtering, grouping, and aggregation.\n",
    "4. Compare the performance of Pandas and Dask for a subset of the data.\n",
    "5. Visualize the results of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "# Generate a large dataset (for demonstration, we'll use a smaller one)\n",
    "n_rows = 10_000_000\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': np.random.randint(1, 100001, n_rows),\n",
    "    'transaction_date': pd.date_range(start='2020-01-01', end='2022-12-31', periods=n_rows),\n",
    "    'amount': np.random.uniform(10, 1000, n_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows)\n",
    "})\n",
    "\n",
    "# Save to CSV (in practice, this would be your large file)\n",
    "df.to_csv('large_transactions.csv', index=False)\n",
    "\n",
    "# Set up Dask DataFrame\n",
    "ddf = dd.read_csv('large_transactions.csv')\n",
    "\n",
    "# Basic exploratory data analysis using Dask\n",
    "print(\"Dask DataFrame Info:\")\n",
    "print(ddf.info())\n",
    "\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(ddf.dtypes)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(ddf.describe().compute())\n",
    "\n",
    "# Data processing pipeline\n",
    "def process_data(df):\n",
    "    # Filter transactions above $500\n",
    "    filtered = df[df['amount'] > 500]\n",
    "    \n",
    "    # Group by category and calculate total amount\n",
    "    grouped = filtered.groupby('category')['amount'].sum().reset_index()\n",
    "    \n",
    "    # Calculate percentage of total for each category\n",
    "    total = grouped['amount'].sum()\n",
    "    grouped['percentage'] = grouped['amount'] / total * 100\n",
    "    \n",
    "    return grouped.sort_values('amount', ascending=False)\n",
    "\n",
    "# Process with Dask\n",
    "start_time = time.time()\n",
    "dask_result = process_data(ddf).compute()\n",
    "dask_time = time.time() - start_time\n",
    "print(\"\\nDask processing result:\")\n",
    "print(dask_result)\n",
    "print(f\"Dask processing time: {dask_time:.2f} seconds\")\n",
    "\n",
    "# Process with Pandas (using a subset of data)\n",
    "pandas_df = pd.read_csv('large_transactions.csv', nrows=1_000_000)\n",
    "start_time = time.time()\n",
    "pandas_result = process_data(pandas_df)\n",
    "pandas_time = time.time() - start_time\n",
    "print(\"\\nPandas processing result (subset):\")\n",
    "print(pandas_result)\n",
    "print(f\"Pandas processing time (subset): {pandas_time:.2f} seconds\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(dask_result['category'], dask_result['percentage'])\n",
    "plt.title('Percentage of Total Amount by Category (Transactions > $500)')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Percentage of Total Amount')\n",
    "plt.show()\n",
    "\n",
    "# Compare performance\n",
    "print(f\"\\nDask processed the entire dataset {n_rows:,} rows in {dask_time:.2f} seconds\")\n",
    "print(f\"Pandas processed a subset of 1,000,000 rows in {pandas_time:.2f} seconds\")\n",
    "print(f\"Estimated time for Pandas to process the entire dataset: {pandas_time * (n_rows / 1_000_000):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the advanced assignment with five questions covering various aspects of data analysis, visualization, and processing using Pandas and related libraries. Each question includes a detailed problem statement and a comprehensive solution that demonstrates advanced techniques in data manipulation, analysis, and visualization.\n",
    "\n",
    "The questions cover:\n",
    "1. Advanced Data Cleaning and Imputation\n",
    "2. Time Series Analysis and Forecasting\n",
    "3. Advanced Data Transformation and Analysis\n",
    "4. Advanced Data Visualization and Statistical Analysis\n",
    "5. Big Data Processing with Pandas and Dask\n",
    "\n",
    "These exercises provide hands-on experience with real-world data challenges and advanced data science techniques using Python and Pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


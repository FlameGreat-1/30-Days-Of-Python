{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Practical Exercises\n",
    "\n",
    "This notebook contains four practical exercises demonstrating advanced Pandas techniques for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data Cleaning Challenge\n",
    "\n",
    "Load a messy dataset, perform comprehensive cleaning, handle missing values, outliers, and inconsistent data types. Create a function to automate the cleaning process for similar datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John', 'Jane', 'Bob', 'Alice', np.nan],\n",
    "    'Age': [25, 30, 'Unknown', 35, 40],\n",
    "    'Salary': ['50,000', '60,000', '55,000', '1,000,000', '45,000'],\n",
    "    'Date': ['2021-01-01', '2021-01-02', '2021/01/03', '2021.01.04', '2021-01-05']\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "def clean_dataset(df):\n",
    "    # Make a copy of the dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean['Name'].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Convert Age to numeric, replacing 'Unknown' with NaN\n",
    "    df_clean['Age'] = pd.to_numeric(df_clean['Age'], errors='coerce')\n",
    "    \n",
    "    # Handle outliers in Age (assuming ages should be between 18 and 100)\n",
    "    df_clean.loc[df_clean['Age'] < 18, 'Age'] = 18\n",
    "    df_clean.loc[df_clean['Age'] > 100, 'Age'] = 100\n",
    "    \n",
    "    # Clean and convert Salary to numeric\n",
    "    df_clean['Salary'] = df_clean['Salary'].str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Handle outliers in Salary (assuming salaries should be between 30,000 and 500,000)\n",
    "    df_clean.loc[df_clean['Salary'] < 30000, 'Salary'] = 30000\n",
    "    df_clean.loc[df_clean['Salary'] > 500000, 'Salary'] = 500000\n",
    "    \n",
    "    # Convert Date to datetime\n",
    "    df_clean['Date'] = pd.to_datetime(df_clean['Date'], format='mixed')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean the dataset\n",
    "df_cleaned = clean_dataset(df)\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned)\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Advanced Data Transformation Project\n",
    "\n",
    "Perform complex reshaping operations on a multi-dimensional dataset, implement advanced groupby operations with custom aggregation functions, and merge multiple datasets handling various join scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "df1 = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='2021-01-01', periods=10),\n",
    "    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'],\n",
    "    'Sales': np.random.randint(100, 1000, 10),\n",
    "    'Quantity': np.random.randint(10, 100, 10)\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C', 'D'],\n",
    "    'Price': [100, 200, 150, 300]\n",
    "})\n",
    "\n",
    "print(\"DataFrame 1:\")\n",
    "print(df1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df2)\n",
    "\n",
    "# Reshape the data: pivot to have products as columns\n",
    "df_pivot = df1.pivot(index='Date', columns='Product', values=['Sales', 'Quantity'])\n",
    "print(\"\\nPivoted DataFrame:\")\n",
    "print(df_pivot)\n",
    "\n",
    "# Custom aggregation function\n",
    "def sales_per_unit(x):\n",
    "    return x['Sales'].sum() / x['Quantity'].sum()\n",
    "\n",
    "# Groupby with custom aggregation\n",
    "df_grouped = df1.groupby('Product').agg({\n",
    "    'Sales': ['sum', 'mean'],\n",
    "    'Quantity': ['sum', 'mean'],\n",
    "    'Date': ['min', 'max']\n",
    "}).assign(SalesPerUnit=df1.groupby('Product').apply(sales_per_unit))\n",
    "\n",
    "print(\"\\nGrouped DataFrame with custom aggregation:\")\n",
    "print(df_grouped)\n",
    "\n",
    "# Merge datasets\n",
    "df_merged = pd.merge(df1, df2, on='Product', how='outer')\n",
    "df_merged['Revenue'] = df_merged['Sales'] * df_merged['Price']\n",
    "\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Time Series Analysis\n",
    "\n",
    "Analyze a financial dataset with stock prices, implement rolling statistics and technical indicators, and perform resampling to different time frequencies handling business days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample stock price dataset\n",
    "dates = pd.date_range(start='2021-01-01', end='2021-12-31', freq='B')\n",
    "prices = np.random.randint(100, 200, size=len(dates)) + np.random.random(size=len(dates))\n",
    "volumes = np.random.randint(1000000, 5000000, size=len(dates))\n",
    "\n",
    "df_stock = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Price': prices,\n",
    "    'Volume': volumes\n",
    "})\n",
    "df_stock.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"Stock price dataset:\")\n",
    "print(df_stock.head())\n",
    "\n",
    "# Calculate rolling statistics\n",
    "df_stock['MA20'] = df_stock['Price'].rolling(window=20).mean()\n",
    "df_stock['MA50'] = df_stock['Price'].rolling(window=50).mean()\n",
    "\n",
    "# Calculate technical indicator: Relative Strength Index (RSI)\n",
    "def calculate_rsi(data, periods=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df_stock['RSI'] = calculate_rsi(df_stock['Price'])\n",
    "\n",
    "print(\"\\nDataFrame with technical indicators:\")\n",
    "print(df_stock.head())\n",
    "\n",
    "# Resample to monthly frequency\n",
    "df_monthly = df_stock.resample('M').agg({\n",
    "    'Price': 'last',\n",
    "    'Volume': 'sum',\n",
    "    'MA20': 'last',\n",
    "    'MA50': 'last',\n",
    "    'RSI': 'last'\n",
    "})\n",
    "\n",
    "print(\"\\nMonthly resampled data:\")\n",
    "print(df_monthly)\n",
    "\n",
    "# Plot the stock price with moving averages\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_stock.index, df_stock['Price'], label='Price')\n",
    "plt.plot(df_stock.index, df_stock['MA20'], label='20-day MA')\n",
    "plt.plot(df_stock.index, df_stock['MA50'], label='50-day MA')\n",
    "plt.title('Stock Price with Moving Averages')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Optimization Challenge\n",
    "\n",
    "Optimize a slow Pandas operation on a large dataset, implement parallel processing for Pandas operations, and profile and compare the performance of different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from numba import jit\n",
    "import multiprocessing\n",
    "\n",
    "# Create a large dataset\n",
    "n = 10000000\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.randint(0, 100, n),\n",
    "    'B': np.random.randint(0, 100, n),\n",
    "    'C': np.random.randint(0, 100, n)\n",
    "})\n",
    "\n",
    "# Slow operation\n",
    "def slow_operation(df):\n",
    "    return df.apply(lambda row: row['A'] * row['B'] + row['C'], axis=1)\n",
    "\n",
    "# Vectorized operation\n",
    "def vectorized_operation(df):\n",
    "    return df['A'] * df['B'] + df['C']\n",
    "\n",
    "# Numba optimized operation\n",
    "@jit(nopython=True)\n",
    "def numba_operation(A, B, C):\n",
    "    return A * B + C\n",
    "\n",
    "# Parallel processing operation\n",
    "def parallel_operation(df):\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    results = pool.map(vectorized_operation, df_split)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Profile and compare performance\n",
    "def profile_operation(operation, df, name):\n",
    "    start_time = time.time()\n",
    "    result = operation(df)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name} took {end_time - start_time:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "# Run and profile operations\n",
    "slow_result = profile_operation(slow_operation, df, \"Slow operation\")\n",
    "vectorized_result = profile_operation(vectorized_operation, df, \"Vectorized operation\")\n",
    "numba_result = profile_operation(lambda df: numba_operation(df['A'].values, df['B'].values, df['C'].values), df, \"Numba operation\")\n",
    "parallel_result = profile_operation(parallel_operation, df, \"Parallel operation\")\n",
    "\n",
    "# Verify results are the same\n",
    "print(f\"\\nAll results are equal: {np.allclose(slow_result, vectorized_result) and np.allclose(slow_result, numba_result) and np.allclose(slow_result, parallel_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These exercises demonstrate advanced Pandas techniques for data cleaning, transformation, time series analysis, and performance optimization. They provide practical experience in handling real-world data challenges and improving code efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
